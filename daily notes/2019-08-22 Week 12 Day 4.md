2019-08-22 Week 12 Day 4   
Unit 3 Sprint 3 Module 4    
Web Application Deployment  

Quinn:lambda-pm: 13:05  
video will be here labeled 334 in approximately... 20-25 minutes i reckon   https://www.dropbox.com/sh/z58fdoo3iyzamfy/AACbDXHuMIr4YEM2IVYXm7z9a?dl=0  

Learning Kit  
https://learn.lambdaschool.com/ds/module/recpPYQdaOmZdBSYq/  

==============================================

```
$mkdir <app folder name> (store your app files there)
$cd <app folder name> (for Google Colab use %cd)
$ls (or ls -lha)
$cd templates
$ls (stores two files in 'templates' folder: index.html, result.html)
```

index.html  
```
<html>
<body>
    <h3>Iris Flower Measurements</h3>
<div>
    <form action="/result" method="POST">
        <label for="SepalLengthCM">Sepal Length CM</label>
        <input type="text" id="SepalLengthCM" name="SepalLengthCM">
        <br>
        <label for="SepalWidthCM">Sepal Width CM</label>
        <input type="text" id="SepalWidthCM" name="SepalWidthCM">
        <br>
        <label for="SepalLengthCM">Sepal Length CM</label>
        <input type="text" id="SepalLengthCM" name="SepalLengthCM">
        <br>
        <label for="SepalLengthCM">Sepal Length CM</label>
        <input type="text" id="SepalLengthCM" name="SepalLengthCM">
        <br>
        <input type="submit" value="Submit">
    </form>
</div>
</body>
</html>
```
result.html  
```
<html>
    <body>
        <h1>The class of Iris flower is: {{ prediction }}</h1>
    </body>
</html>
```

heroku_app.py
```
import os
import numpy as np
from flask import Flask, render_template, request
import pickle

def ValuePredictor(to_predict_list):
    to_predict = np.array(to_predict_list).reshape(1,-1)
    loaded_model = pickle.load(open("iris_model2.pkl", "rb"))
    result = loaded_model.predict(to_predict)
    return result[0]

app = Flask(__name__)

@app.route('/')
@app.route('/index')
def index():
    return flask.render_template('index.html')

@app.route('/result', methods=['POST'])
def result():
    if request.method == 'POST':
        to_predict_list = request.form.to_dict()
        to_predict_list = list(to_predict_list.values())
        to_predict_list = list(map(int, to_predict_list))
 
        result = ValuePredictor(to_predict_list)  
        if result == 0:
            result = 'Setosa'
        elif result == 1:
            result = 'Versicolor'
        else:   
            result = 'Virginica'
    return render_template("results.html", prediction=result)

if __name__ == '__main__':
    app.run(port=9000, debug=True)
```
1. install in your env: gunicorn    
https://gunicorn.org/   
handles requests and threading  
waitress is an alternative  
`pipenv install gunicorn`   
 
Ben Manning 12:41  
https://stackoverflow.com/questions/11087682/does-gunicorn-run-on-windows

2. `pip freeze > requirements.txt`   
`conda list -e > requirements.txt` (Anaconda)    

3. Procfile  
`web: gunicorn heroku_app:app` (heroku_app=name of the app file)  
`web: gunicorn --chdir <appfilename>:app`
for Windows (not in a virtual environment):   
`waitress-serve --listen=*:8000 myapp.wsgi:application`    

Reflected in deployment logs:   
```
-----> Discovering process types
       Procfile declares types -> web
```  

4. optional `.gitignore` - ignore files listed in this file  

5. debug   
Chance Dare Today at 13:05   
For debugging do `heroku logs --tail`   
Aaron Gallant:lambda-red:  9 minutes ago  
Or `heroku run flask shell` or similar can also help (assuming your environment is set up that far).   
Aaron Gallant:lambda-red:  8 minutes ago  
`heroku run /bin/bash` will just drop you in a command line in the Heroku server.  

log example:  
https://github.com/Nov05/Lambda-School-Data-Science/blob/master/miscellaneous/2019-08-01%20heroku%20deployment%20logs.md  


