2019-09-10 Week 15 Day 2  
Unit 4 Sprint 2 Module 2  
Gradient Descent and Backpropagation      

Lecture video  
<to be updated>  

Lecture notebook   
https://drive.google.com/file/d/1Xd7M6Hk2PWyJdXvRPd6aqkzsgQd0vcqa/view?usp=sharing  

Traning Kit   
https://learn.lambdaschool.com/ds/module/recU22sI50giR3dES/      

Repo for this sprint  
https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks  

=====================================

Jon-Cody Sokoll 11:18   
https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.19552&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false   
playground.tensorflow.orgplayground.tensorflow.org   
Tensorflow â€” Neural Network Playground  

```
nn = NeuralNetwork()
for i in range(1000):
    if (i+1 in [1,2,3,4,5]) or ((i+1) % 50 ==0):
        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')
        print('Input: \n', X)
        print('Actual Output: \n', y)
        print('Predicted Output: \n', str(nn.feed_forward(X)))
        print("Loss: \n", str(np.mean(np.square(y - nn.feed_forward(X)))))
    nn.train(X,y)
```

```
class NeuralNetwork: 
    def __init__(self):
        # Set upArchietecture 
        self.inputs = 2
        self.hiddenNodes = 3
        self.outputNodes = 1
        
        #Initial weights
        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #2x3
        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #3x1
    
    def sigmoid(self, s):
        return 1 / (1+np.exp(-s))
    
    def sigmoidPrime(self, s):
        return s * (1 - s)
    
    def feed_forward(self, X):
        """
        Calculate the NN inference using feed forward.
        """
        
        #Weighted sume of inputs and hidden layer
        self.hidden_sum = np.dot(X, self.weights1)
        
        #Acivations of weighted sum
        self.activated_hidden = self.sigmoid(self.hidden_sum)
        
        # Weight sum between hidden and output
        self.output_sum = np.dot(self.activated_hidden, self.weights2)
        
        #Final activation of output
        self.activated_output = self.sigmoid(self.output_sum)
        
        return self.activated_output
    
    def backward(self, X, y, o):
        """
        Backward propagate through the network
        """
        self.o_error = y - o #error in output
        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error
        
        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off
        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)
        
        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights
        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights
        
    def train(self, X, y):
        o = self.feed_forward(X)
        self.backward(X, y, o)
```

**Build Week**

Elizabeth Ter Sahakyan 13:00   
https://airtable.com/shrrrSCvsqJi312kg   
If you want to submit a new project   

Jon-Cody Sokoll 13:00   
https://lambdaschoolstudents.slack.com/files/UJ5308HL6/FM0SB5JRG/build_week_project_idea_guidance_for_data_science_students    

**Code Challenge**  
https://colab.research.google.com/drive/17scEysWpqa4Ni47De-chi1njAHAZILmm  

